# **Multi-Channel Sentiment Analysis System** ğŸ­ğŸ¤ğŸ“  

This project implements a multi-channel sentiment analysis system that integrates **facial expressions**, **voice tone**, and **speech transcription** to analyze human emotions. A **voting mechanism** is employed to combine results, offering transparency, modularity, and robustness compared to traditional multimodal systems.

---

## **Abstract** ğŸ“„  
This system independently analyzes sentiment across three modalities:
- **Facial Expressions** (FERPlus, OpenFace)
- **Voice Tone** (Wav2Vec2 fine-tuned for RAVDESS)
- **Speech Transcriptions** (DistilRoBERTa for emotion detection)

A **post-processing voting mechanism** is applied to combine outputs. Unlike black-box neural networks, this approach is interpretable and adaptable, allowing conflict resolution between emotional signals for a more accurate and explainable sentiment classification.

---

## **System Workflow** ğŸš€  
The system processes a video file (30-second segments) and performs the following steps:

1. **Frame Extraction**:
   - Convert video into frames at **1-second intervals** for facial expression analysis.  
     ğŸ“œ Refer: [FacialExpression.md](./docs/FacialExpression.md)  

2. **Speech and Voice Analysis**:
   - Extract audio for **voice tone** and **speech transcription**.  

3. **Modality-Specific Analysis**:
   - **Facial Expression**: Detect frame-level emotions using **Facetorch** or **OpenFace**.
   - **Voice Tone**: Analyze audio tone with **Wav2Vec2**.
   - **Speech Transcription**: Sentiment detection using **DistilRoBERTa**.

4. **Syncing and Voting**:
   - Align outputs across timestamps and apply a weighted **voting mechanism** for final sentiment classification.  
     ğŸ“œ Refer: [syncing.md](./docs/syncing.md)  

---

## **Voting Mechanism** ğŸ—³ï¸  
The **voting mechanism** assigns dynamic weights to each channel based on their reliability and context:  

| **Modality**            | **Weight** | **Reason**                                                 |
|--------------------------|------------|-----------------------------------------------------------|
| **Speech Transcription** | 0.5        | Clear, context-rich emotional cues.                       |
| **Facial Expression**    | 0.3        | Universally understood cues; reliable for static data.    |
| **Voice Tone**           | 0.2        | Prone to ambiguity; complementary to other channels.      |

**Conflict Resolution**:
- **Positive vs Negative Conflict**: Adjust weights proportionally to conflict ratio.
- **Inter-Chunk Outlier Handling**: Replace short, transient emotion states with "Neutral" for smoother sequences.  
  ğŸ“œ Refer: [inter-chunk.md](./docs/inter-chunk.md).

---


## **Project Structure** ğŸ“‚
```
THESIS-SEQUENCE-SYNCING-SYSTEM/
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ advantage.md             # Advantages of Voting Mechanism
â”‚   â”œâ”€â”€ CN3-DES400.pdf           # Thesis Report
â”‚   â”œâ”€â”€ FacialExpression.md      # Facial Expression Model Notes
â”‚   â”œâ”€â”€ inter-chunk.md           # Inter-Chunk Syncing Details
â”‚   â”œâ”€â”€ Script-DES400.pdf        # Implementation Notes
â”‚   â”œâ”€â”€ syncing.md               # Syncing Mechanism Explanation
â”‚   â”œâ”€â”€ Syncing.pdf              # Syncing Report
â”‚   â””â”€â”€ test_dataset.md          # Test Dataset Info
â”‚
â”œâ”€â”€ facial_expression/           # Facial Expression Analysis
â”‚   â”œâ”€â”€ FERPlus/                 # FERPlus Dataset & Scripts
â”‚   â”‚   â”œâ”€â”€ data/                # Processed Data
â”‚   â”‚   â”‚   â”œâ”€â”€ input/           # Raw Input Data
â”‚   â”‚   â”‚   â”œâ”€â”€ jaffe/           # JAFFE Dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ jaffe_converted/ # Converted JAFFE Data
â”‚   â”‚   â”‚   â”œâ”€â”€ labels/          # Emotion Labels
â”‚   â”‚   â”‚   â”œâ”€â”€ models/          # Trained Models
â”‚   â”‚   â”‚   â”œâ”€â”€ output/          # Outputs
â”‚   â”‚   â”‚   â””â”€â”€ video_frames/    # Extracted Frames
â”‚   â”‚   â”œâ”€â”€ facetorch/           # Facetorch Source Files
â”‚   â”‚   â”œâ”€â”€ notebooks/           # Jupyter Notebooks
â”‚   â”‚   â”œâ”€â”€ scripts/             # Helper Scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_features.sh
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze_openface_ferplus.py
â”‚   â”‚   â”‚   â””â”€â”€ validate_all.bat
â”‚   â”‚   â”œâ”€â”€ environment.yml      # Environment Configurations
â”‚   â”‚   â”œâ”€â”€ README.md            # FERPlus Documentation
â”‚   â”‚   â””â”€â”€ .gitattributes       # Git Attributes
â”‚   â”‚
â”‚   â””â”€â”€ OpenFace/                # OpenFace Facial Analysis
â”‚       â”œâ”€â”€ data/                # Input & Output Data
â”‚       â”‚   â”œâ”€â”€ labels/
â”‚       â”‚   â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ OpenFace/            # OpenFace Source Files
â”‚       â”‚   â”œâ”€â”€ build/
â”‚       â”‚   â”œâ”€â”€ lib/
â”‚       â”‚   â”œâ”€â”€ imgs/
â”‚       â”‚   â”œâ”€â”€ packages/
â”‚       â”‚   â”œâ”€â”€ python_scripts/
â”‚       â”‚   â”œâ”€â”€ matlab_runners/
â”‚       â”‚   â””â”€â”€ model_training/
â”‚       â”œâ”€â”€ README.md            # OpenFace Notes
â”‚       â”œâ”€â”€ analyze_openface.py  # OpenFace Execution Script
â”‚       â””â”€â”€ extract_features.sh
â”‚
â”œâ”€â”€ input_sample_video/          # Input Videos for Testing
â”‚   â”œâ”€â”€ negative (angry) - penguinz0/
â”‚   â”‚   â”œâ”€â”€ sound/               # Extracted Sound Files
â”‚   â”‚   â”œâ”€â”€ video/               # Video Segments
â”‚   â”‚   â””â”€â”€ youtube link.url
â”‚   â”œâ”€â”€ negative (sad) - logan paul/
â”‚   â”œâ”€â”€ negative (sad) - markiplier part 1/
â”‚   â”œâ”€â”€ neutral - mrballen/
â”‚   â”œâ”€â”€ positive (happy) - tommyinnit/
â”‚   â””â”€â”€ positive (sarcastic) - michael reeves/
â”‚
â”œâ”€â”€ voice_tone/                  # Voice Tone Emotion Analysis
â”‚   â”œâ”€â”€ ...                      # Placeholder for Voice Tone Scripts
â”‚
â”œâ”€â”€ voice_transcription/         # Speech-to-Text Sentiment Analysis
â”‚   â”œâ”€â”€ ...                      # Placeholder for Transcription Scripts
â”‚
â”œâ”€â”€ .gitignore                   # Git Ignore Configuration
â”œâ”€â”€ output.json                  # Final Output Results
â”œâ”€â”€ README.md                    # Project Documentation
â”œâ”€â”€ requirements.txt             # Python Dependencies
â”œâ”€â”€ syncing.py                   # Syncing Logic Script
â””â”€â”€ voting.py                    # Voting Mechanism Script
```

## **Installation** âš™ï¸  
1. **Clone Repository**:
   ```bash
   git clone https://github.com/your_project_repo.git
   cd your_project_repo
   ```

2. **Setup Environment**:
   ```bash
   conda create -n sentiment_env python=3.9 -y
   conda activate sentiment_env
   pip install -r requirements.txt
   ```

3. **Install Dependencies**:
   - For **Facial Analysis**: Follow instructions in [FERPlus README](./facial_expression/FERPlus/README.md).
   - For **OpenFace**: Refer to [OpenFace Installation](./facial_expression/OpenFace/README.md).

---

## **How to Run** â–¶ï¸

1. **Preprocess Video**:
   ```bash
   python scripts/extract_frames.py --input input_sample_video/video.mp4 --output data/frames/
   ```

2. **Run Individual Channels**:
   - Facial Expression Analysis:
     ```bash
     python facial_expression/FERPlus/scripts/analyze_openface.py --input data/frames/ --output data/outputs/facial.json
     ```
   - Voice Tone Analysis:
     ```bash
     python voice_tone/analyze_tone.py --input input_sample_video/video.mp4 --output data/outputs/voice.json
     ```
   - Speech Transcription Analysis:
     ```bash
     python voice_transcription/analyze_transcript.py --input input_sample_video/video.mp4 --output data/outputs/transcript.json
     ```

3. **Combine Results Using Voting**:
   ```bash
   python scripts/voting.py --facial data/outputs/facial.json \
                            --voice data/outputs/voice.json \
                            --transcript data/outputs/transcript.json \
                            --output data/results/final.json
   ```

---

## **Future Work** ğŸ”®  
The method demonstrates improvements over single-channel approaches but has room for enhancements:
1. **Sarcastic Detection**: Analyze **contradictory cues** like happy expressions with angry tone.
2. **Hidden Sadness**: Combine subtle signals (neutral expression, sad voice tone) to detect layered emotions.
3. **Real-Time Processing**: Adapt the system for live-streamed video analysis.

---

## **Results** ğŸ“Š  
- The system outputs a JSON file with final sentiment classification for each time partition:  
  ```json
  [
      {"partition": "0-6s", "emotion": "Neutral", "confidence": 0.82},
      {"partition": "6-12s", "emotion": "Happy", "confidence": 0.91}
  ]
  ```

- Comparative analysis with single-channel approaches shows improved accuracy, particularly in ambiguous and noisy contexts.  
ğŸ“œ Full evaluation: [CN3-DES400.pdf](./docs/CN3-DES400.pdf)

---

## **References** ğŸ“š  
- Final Report: [CN3-DES400.pdf](./docs/CN3-DES400.pdf)  
- Syncing Logic: [syncing.md](./docs/syncing.md)  
- Facial Expression: [FacialExpression.md](./docs/FacialExpression.md)

---

Let me know if you'd like additional sections or refinements! ğŸš€