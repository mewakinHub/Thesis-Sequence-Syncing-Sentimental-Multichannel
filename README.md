# **Multi-Channel Sentiment Analysis with Voting Mechanism** ğŸ­ğŸ¤ğŸ“  
This repository provides an implementation of a **multi-channel sentiment analysis** system that uses **facial expression**, **voice tone**, and **speech transcription** to classify human emotions. By employing a **voting mechanism** instead of traditional multimodal fusion, the system ensures flexibility, transparency, and adaptability to dynamic scenarios.

---

## **Purpose** ğŸ“š
Traditional **single-channel sentiment analysis** and **early fusion multimodal systems** face challenges in handling:
1. Noise in real-world data.
2. Conflicts between emotional signals.
3. Lack of interpretability (e.g., black-box neural networks).

This project introduces a **post-processing voting mechanism** that:
- Combines independent outputs from multiple channels.
- Resolves conflicts with dynamic thresholds and conditions.
- Enhances interpretability and modularity.

ğŸ”— **Detailed Explanation**:  
- [Advantages of Voting Mechanism](./docs/advantage.md)  
- [Facial Expression Analysis](./docs/FacialExpression.md)  
- [Syncing Mechanism](./docs/syncing.md)  

---

## **Features** ğŸš€
1. **Facial Expression Analysis**:
   - Use models like **Facetorch (FER-like)** and **OpenFace** for static and dynamic frame analysis.
   - Outputs per-frame emotions with timestamps.  
   â¡ï¸ [FacialExpression.md](./docs/FacialExpression.md)

2. **Voice Tone Analysis**:
   - Utilize **Wav2Vec2** models for speech emotion recognition.
   - Detects vocal emotions such as **Happy, Sad, Angry, Calm, Neutral**.  

3. **Speech Transcription Sentiment**:
   - Transcribe speech using **Google Speech-to-Text** or **Whisper**.
   - Analyze sentiment using **DistilRoBERTa** for **positive, negative, and neutral** classifications.

4. **Voting Mechanism**:
   - Combines results from all channels after independent analysis.
   - Dynamically resolves conflicts and handles edge cases.  
   â¡ï¸ [Syncing.md](./docs/syncing.md)

---

## **Project Structure** ğŸ“‚



```
THESIS-SEQUENCE-SYNCING-SYSTEM/
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ advantage.md             # Advantages of Voting Mechanism
â”‚   â”œâ”€â”€ CN3-DES400.pdf           # Thesis Report
â”‚   â”œâ”€â”€ FacialExpression.md      # Facial Expression Model Notes
â”‚   â”œâ”€â”€ inter-chunk.md           # Inter-Chunk Syncing Details
â”‚   â”œâ”€â”€ Script-DES400.pdf        # Implementation Notes
â”‚   â”œâ”€â”€ syncing.md               # Syncing Mechanism Explanation
â”‚   â”œâ”€â”€ Syncing.pdf              # Syncing Report
â”‚   â””â”€â”€ test_dataset.md          # Test Dataset Info
â”‚
â”œâ”€â”€ facial_expression/           # Facial Expression Analysis
â”‚   â”œâ”€â”€ FERPlus/                 # FERPlus Dataset & Scripts
â”‚   â”‚   â”œâ”€â”€ data/                # Processed Data
â”‚   â”‚   â”‚   â”œâ”€â”€ input/           # Raw Input Data
â”‚   â”‚   â”‚   â”œâ”€â”€ jaffe/           # JAFFE Dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ jaffe_converted/ # Converted JAFFE Data
â”‚   â”‚   â”‚   â”œâ”€â”€ labels/          # Emotion Labels
â”‚   â”‚   â”‚   â”œâ”€â”€ models/          # Trained Models
â”‚   â”‚   â”‚   â”œâ”€â”€ output/          # Outputs
â”‚   â”‚   â”‚   â””â”€â”€ video_frames/    # Extracted Frames
â”‚   â”‚   â”œâ”€â”€ facetorch/           # Facetorch Source Files
â”‚   â”‚   â”œâ”€â”€ notebooks/           # Jupyter Notebooks
â”‚   â”‚   â”œâ”€â”€ scripts/             # Helper Scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ extract_features.sh
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze_openface_ferplus.py
â”‚   â”‚   â”‚   â””â”€â”€ validate_all.bat
â”‚   â”‚   â”œâ”€â”€ environment.yml      # Environment Configurations
â”‚   â”‚   â”œâ”€â”€ README.md            # FERPlus Documentation
â”‚   â”‚   â””â”€â”€ .gitattributes       # Git Attributes
â”‚   â”‚
â”‚   â””â”€â”€ OpenFace/                # OpenFace Facial Analysis
â”‚       â”œâ”€â”€ data/                # Input & Output Data
â”‚       â”‚   â”œâ”€â”€ labels/
â”‚       â”‚   â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ OpenFace/            # OpenFace Source Files
â”‚       â”‚   â”œâ”€â”€ build/
â”‚       â”‚   â”œâ”€â”€ lib/
â”‚       â”‚   â”œâ”€â”€ imgs/
â”‚       â”‚   â”œâ”€â”€ packages/
â”‚       â”‚   â”œâ”€â”€ python_scripts/
â”‚       â”‚   â”œâ”€â”€ matlab_runners/
â”‚       â”‚   â””â”€â”€ model_training/
â”‚       â”œâ”€â”€ README.md            # OpenFace Notes
â”‚       â”œâ”€â”€ analyze_openface.py  # OpenFace Execution Script
â”‚       â””â”€â”€ extract_features.sh
â”‚
â”œâ”€â”€ input_sample_video/          # Input Videos for Testing
â”‚   â”œâ”€â”€ negative (angry) - penguinz0/
â”‚   â”‚   â”œâ”€â”€ sound/               # Extracted Sound Files
â”‚   â”‚   â”œâ”€â”€ video/               # Video Segments
â”‚   â”‚   â””â”€â”€ youtube link.url
â”‚   â”œâ”€â”€ negative (sad) - logan paul/
â”‚   â”œâ”€â”€ negative (sad) - markiplier part 1/
â”‚   â”œâ”€â”€ neutral - mrballen/
â”‚   â”œâ”€â”€ positive (happy) - tommyinnit/
â”‚   â””â”€â”€ positive (sarcastic) - michael reeves/
â”‚
â”œâ”€â”€ voice_tone/                  # Voice Tone Emotion Analysis
â”‚   â”œâ”€â”€ ...                      # Placeholder for Voice Tone Scripts
â”‚
â”œâ”€â”€ voice_transcription/         # Speech-to-Text Sentiment Analysis
â”‚   â”œâ”€â”€ ...                      # Placeholder for Transcription Scripts
â”‚
â”œâ”€â”€ .gitignore                   # Git Ignore Configuration
â”œâ”€â”€ output.json                  # Final Output Results
â”œâ”€â”€ README.md                    # Project Documentation
â”œâ”€â”€ requirements.txt             # Python Dependencies
â”œâ”€â”€ syncing.py                   # Syncing Logic Script
â””â”€â”€ voting.py                    # Voting Mechanism Script
```

---

## **Installation** âš™ï¸

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your_project_repo.git
   cd your_project_repo
   ```

2. **Set Up Environment**:
   ```bash
   conda create -n sentiment_env python=3.9 -y
   conda activate sentiment_env
   pip install -r requirements.txt
   ```

3. **Install OpenFace and Facetorch**:
   Follow the guides in [FacialExpression.md](./docs/FacialExpression.md).

---

## **How to Run** â–¶ï¸

1. **Extract Frames for Facial Analysis**:
   ```bash
   python scripts/extract_frames.py --input ./input_sample_video/video.mp4 --output ./data/frames/
   ```

2. **Run Facial, Voice, and Text Analysis**:
   - **Facial Expression**:
     ```bash
     python scripts/analyze_openface.py --input ./data/frames/ --output ./data/outputs/facial_results.json
     ```
   - **Voice Tone**:
     ```bash
     python voice_tone/analyze_tone.py --input ./input_sample_video/video.mp4 --output ./data/outputs/voice_results.json
     ```
   - **Speech Transcription**:
     ```bash
     python voice_transcription/analyze_transcript.py --input ./input_sample_video/video.mp4 --output ./data/outputs/transcript_results.json
     ```

3. **Run the Voting Mechanism**:
   ```bash
   python scripts/voting.py --facial ./data/outputs/facial_results.json \
                            --voice ./data/outputs/voice_results.json \
                            --transcript ./data/outputs/transcript_results.json \
                            --output ./data/results/final_sentiment.json
   ```

4. **Review Results**:
   The final sentiment results will be saved in `./data/results/final_sentiment.json`.

---

## **Results** ğŸ“Š

Sample Output:
```json
[
    {
        "partition": "0-6s",
        "final_emotion": "Neutral",
        "confidence": 0.85
    },
    {
        "partition": "6-12s",
        "final_emotion": "Happy",
        "confidence": 0.92
    },
    {
        "partition": "12-18s",
        "final_emotion": "Conflicted",
        "confidence": 0.45
    }
]
```

---

## **Advantages** ğŸ†  
1. **Flexibility**: Easily replace or update models for each channel.  
2. **Noise Tolerance**: Filters inconsistent results using thresholds and stability conditions.  
3. **Modular Design**: Each channel operates independently, improving scalability.  
4. **Transparency**: Enhances interpretability compared to black-box neural networks.

â¡ï¸ **More Details**: [Advantages of Voting](./docs/advantage.md)

---

## **Future Work** ğŸ”®
- **Real-Time Processing**: Extend for live video inputs.  
- **Advanced Conflict Resolution**: Handle sarcastic and mixed emotions.  
- **Improved Accuracy**: Fine-tune models with real-world emotional datasets.

---

## **Contributing** ğŸ¤
Feel free to fork, raise issues, or contribute to this project.  
Refer to the guidelines in [CONTRIBUTING.md](./CONTRIBUTING.md).

---

## **References** ğŸ“š
1. **Thesis Report**: [Full Report](./docs/CN3-DES400.pdf)  
2. **Facial Analysis**: [FacialExpression.md](./docs/FacialExpression.md)  
3. **Syncing Logic**: [Syncing.md](./docs/syncing.md)

---

## **License** ğŸ“„
This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

